'''
Script to download news from Agència Calatana de Notícies (ACN) website
using Selenium IDE and Selenium Web Driver for Firefox.

This scrapper is prepared to run without graphical interface, using the chrome
driver and faking a screen via a python wrapper for Xvfb.

For each article, the script checks if that article has been previously
downloaded, so it can be executed in different moments avoiding duplicates.

https://addons.mozilla.org/en-US/firefox/addon/selenium-ide/
https://github.com/mozilla/geckodriver/releases/download/v0.27.0/geckodriver-v0.27.0-linux64.tar.gz

Usage: pytest -s test_acn.py
Note: -s (--capture=no) option allows to see stdout like print statements inside test_* functions.

Author: https://github.com/aasensios
'''

# Generated by Selenium IDE
import pytest
import time
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support import expected_conditions
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from selenium.common.exceptions import NoSuchElementException

import os
from pathlib import Path
from pyvirtualdisplay import Display

CHROMEDRIVER_PATH = os.path.join(Path().absolute(), 'chromedriver_linux64_85.0.4183.87', 'chromedriver')
DOWNLOADS_DIR = os.path.join(Path().absolute(), 'downloaded_files')
BASE_URL = 'https://www.acn.cat'
USERNAME = 'TEXT'
PASSWORD = '1865GB'

class TestAcn():
    def setup_method(self, method):
        self.vars = {}

        # https://blog.testproject.io/2018/02/20/chrome-headless-selenium-python-linux-servers/
        chrome_options = webdriver.ChromeOptions()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox') # Required when running as root user; otherwise you would get no sandbox errors. 
        prefs = {
            "download.default_directory": DOWNLOADS_DIR,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
        }
        chrome_options.add_experimental_option('prefs', prefs)

        # Start a virtual display before lanching Chrome.
        Display().start()

        self.driver = webdriver.Chrome(
            executable_path=CHROMEDRIVER_PATH,
            options=chrome_options,
            service_args=['--verbose', '--log-path=./chromedriver.log']
        )

    def teardown_method(self, method):
        self.driver.quit()

    def test_download_news_with_metadata(self):

        # Login
        self.driver.get(f"{BASE_URL}/subscriptors")
        self.driver.find_element_by_id("username").send_keys(USERNAME)
        self.driver.find_element_by_id("password").send_keys(PASSWORD)
        self.driver.find_element_by_xpath("//button[@type=\'submit\']").click()

        # Accept cookies in bottom banner to avoid 'selenium.common.exceptions.ElementClickInterceptedException'.
        self.driver.find_element_by_class_name('accept').click()

        # Go to section where news can be downloaded as plain text files.
        self.driver.get(f"{BASE_URL}/text")
        while True:
            current_page_url = self.driver.current_url

            # Terminal feedback on currently precessed page.
            print('')
            print(current_page_url)

            article_elements = self.driver.find_elements_by_xpath("//a[starts-with(@href, '/text/item')]")
            articles_urls = [element.get_attribute("href") for element in article_elements]
            for article_url in articles_urls:

                # Load the article in emulated browser.
                self.driver.get(article_url)

                # Get the article id.
                id = self.driver.find_elements_by_class_name('element-staticcontent')[1].text.split(': ')[1]
                txt_file_path = os.path.join(DOWNLOADS_DIR, f'noticia_{id}.txt')

                # Avoid downloading and parsing an article more than once.
                if os.path.isfile(txt_file_path):
                    continue

                # Download the txt file.
                self.driver.find_element_by_xpath("//a[starts-with(@id, 'download')]").click()

                # Get metadata.
                publication_datetime = self.driver.find_element_by_css_selector(".uk-text-left > .uk-margin-small").text
                section_subsection = self.driver.find_elements_by_class_name('element-relatedcategories')[0].text.split(': ')[1].split(', ')
                section = section_subsection[0]
                subsection = section_subsection[1] if len(section_subsection) > 1 else None
                territorial_coding = self.driver.find_elements_by_class_name('element-relatedcategories')[1].text.split(': ')[1].split(', ')
                categories = self.driver.find_element_by_class_name('element-itemcategory').text.split(': ')[1].split(', ')

                # Some articles may not have assigned tags.
                try:
                    tags = self.driver.find_element_by_class_name('element-itemtag').text.replace('Etiquetes', 'Etiquetes:').split(': ')[1].split(', ')  # Fix missing colon ':'.
                except NoSuchElementException:
                    tags = list()

                # Get plain text content from the downloaded file knowing the article's id.
                with open(txt_file_path) as f:
                    text = f.read()
                    title = text.splitlines()[0]
                    subtitle = text.splitlines()[1]
                    body = '\n'.join(line for line in text.splitlines()[2:] if line)

                metadata = dict(
                    url=article_url,
                    publication_datetime=publication_datetime,
                    text=text,
                    title=title,
                    subtitle=subtitle,
                    body=body,
                    section=section,
                    subsection=subsection,
                    territorial_coding=territorial_coding,
                    categories=categories,
                    id=id,
                    tags=tags,
                )

                # Write metadata and text in a json file.
                with open(os.path.join(DOWNLOADS_DIR, f'noticia_{id}.json'), 'w') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)

                # Terminal feedback on currently processed article.
                print(article_url)

            # Go back to current page and look for next page button or finish if last page.
            self.driver.get(current_page_url)
            if not self.driver.find_elements_by_link_text("»"):
                break
            self.driver.find_element_by_link_text("»").click()

        # Logout
        self.driver.find_element_by_link_text("Surt").click()
